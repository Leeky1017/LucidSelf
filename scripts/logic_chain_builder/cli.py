"""
CLI - å‘½ä»¤è¡Œæ¥å£

æä¾›Logic Chain Builderçš„å‘½ä»¤è¡Œå…¥å£ã€‚

Usage:
    python -m scripts.logic_chain_builder build <book_id>
    python -m scripts.logic_chain_builder batch
    python -m scripts.logic_chain_builder validate <book_id>
    python -m scripts.logic_chain_builder list
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Optional

import yaml

from scripts.logic_chain_builder.constants import TARGET_BOOKS, OUTPUT_DIR
from scripts.logic_chain_builder.logging_config import setup_logging, get_logger
from scripts.logic_chain_builder.builder import LogicChainBuilder
from scripts.logic_chain_builder.validator import LogicChainValidator
from scripts.logic_chain_builder.writer import LogicChainWriter
from scripts.logic_chain_builder.loader import SchemaLoader, SchemaLoadError
from scripts.logic_chain_builder.models import LogicChain


def cmd_build(args) -> int:
    """
    æ„å»ºå•æœ¬ä¹¦ç±çš„é€»è¾‘é“¾
    
    Wire up LogicChainBuilder, LogicChainValidator, and LogicChainWriter.
    Build logic chain for single book, validate and auto-repair if needed,
    then write output to YAML file.
    
    **Validates: Requirements 1.1, 1.2, 7.1**
    """
    logger = get_logger()
    book_id = args.book_id
    
    logger.info(f"ğŸ“š å¼€å§‹æ„å»ºé€»è¾‘é“¾: {book_id}")
    
    try:
        # 1. Build the logic chain
        builder = LogicChainBuilder(book_id)
        chain = builder.build()
        logger.info(f"âœ… æ„å»ºå®Œæˆ: {len(chain.nodes)} èŠ‚ç‚¹, {len(chain.edges)} è¾¹")
        
        # 2. Validate the chain
        validator = LogicChainValidator()
        result = validator.validate(chain)
        
        if result.warnings:
            for warning in result.warnings:
                logger.warning(f"âš ï¸  {warning}")
        
        # 3. Auto-repair if validation failed
        if not result.valid:
            logger.warning(f"âŒ éªŒè¯å¤±è´¥ï¼Œå‘ç° {len(result.errors)} ä¸ªé”™è¯¯ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...")
            for error in result.errors:
                logger.warning(f"   - {error.error_type}: {error.message}")
            
            chain, repairs = validator.auto_repair(chain, result.errors)
            
            if repairs:
                logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤å®Œæˆï¼Œå…± {len(repairs)} é¡¹ä¿®å¤:")
                for repair in repairs:
                    logger.info(f"   - {repair}")
            
            # Re-validate after repair
            result = validator.validate(chain)
            if not result.valid:
                logger.error("âŒ è‡ªåŠ¨ä¿®å¤åä»æœ‰éªŒè¯é”™è¯¯:")
                for error in result.errors:
                    logger.error(f"   - {error.error_type}: {error.message}")
                return 1
            else:
                logger.info("âœ… è‡ªåŠ¨ä¿®å¤åéªŒè¯é€šè¿‡")
        else:
            logger.info("âœ… éªŒè¯é€šè¿‡")
        
        # 4. Write output to YAML file
        writer = LogicChainWriter()
        output_path = writer.write(chain)
        logger.info(f"ğŸ“ å·²å†™å…¥: {output_path}")
        
        # 5. Print summary
        print(f"\n{'='*60}")
        print(f"é€»è¾‘é“¾æ„å»ºå®Œæˆ: {book_id}")
        print(f"{'='*60}")
        print(f"  èŠ‚ç‚¹æ•°: {len(chain.nodes)}")
        print(f"  è¾¹æ•°: {len(chain.edges)}")
        print(f"  å…¥å£èŠ‚ç‚¹: {len(chain.entry_nodes)}")
        print(f"  ç»ˆç«¯èŠ‚ç‚¹: {len(chain.terminal_nodes)}")
        print(f"  è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"{'='*60}\n")
        
        return 0
        
    except FileNotFoundError as e:
        logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        return 1
    except SchemaLoadError as e:
        logger.error(f"âŒ SchemaåŠ è½½é”™è¯¯: {e}")
        return 1
    except Exception as e:
        logger.error(f"âŒ æ„å»ºå¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return 1


def cmd_batch(args) -> int:
    """
    æ‰¹é‡æ„å»ºæ‰€æœ‰ä¹¦ç±çš„é€»è¾‘é“¾
    
    Batch orchestration for all 21 books:
    - Scan data/schema_staging/snippets/ for all book_ids
    - Verify each book has both snippets and relations files
    - Process all 21 books sequentially
    - Continue on individual book failure
    - Collect success/failure status for each book
    - Generate BuildReport with all statistics
    - Verify exactly 21 logic chain files exist after batch run
    
    **Validates: Requirements 8.1, 8.2, 9.1, 9.2, 9.3, 9.4, 9.5, 9.6**
    """
    logger = get_logger()
    
    # Import report generator and quality scorer for batch processing
    from scripts.logic_chain_builder.report import BuildReportGenerator
    from scripts.logic_chain_builder.models import BookStats, BookQualityScores
    from scripts.logic_chain_builder.quality_scorer import LogicChainQualityScorer
    
    logger.info(f"ğŸ“š å¼€å§‹æ‰¹é‡æ„å»ºé€»è¾‘é“¾ï¼Œå…± {len(TARGET_BOOKS)} æœ¬ä¹¦ç±")
    
    # Initialize components
    loader = SchemaLoader()
    validator = LogicChainValidator()
    writer = LogicChainWriter()
    report_generator = BuildReportGenerator()
    quality_scorer = LogicChainQualityScorer()
    
    # Step 1: Discover all book_ids from snippets directory (Requirement 9.1)
    available_books = loader.list_available_books()
    logger.info(f"å‘ç° {len(available_books)} æœ¬å¯å¤„ç†çš„ä¹¦ç±ï¼ˆåŒæ—¶æ‹¥æœ‰snippetså’Œrelationsæ–‡ä»¶ï¼‰")
    
    # Step 2: Verify each book_id has both snippets and relations files (Requirement 9.2)
    missing_books = []
    for book_id in TARGET_BOOKS:
        if book_id not in available_books:
            missing_books.append(book_id)
            logger.warning(f"ä¹¦ç± {book_id} ç¼ºå°‘snippetsæˆ–relationsæ–‡ä»¶")
    
    if missing_books:
        logger.warning(f"å…± {len(missing_books)} æœ¬ä¹¦ç±ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing_books}")
    
    # Track statistics
    book_stats_list = []
    successful = 0
    failed = 0
    skipped = 0
    
    # Progress bar width
    bar_width = 40
    
    print(f"\n{'='*60}")
    print(f"æ‰¹é‡æ„å»ºé€»è¾‘é“¾ - {len(TARGET_BOOKS)} æœ¬ä¹¦ç±")
    print(f"{'='*60}")
    print(f"å¯å¤„ç†ä¹¦ç±: {len(available_books)}/{len(TARGET_BOOKS)}")
    print(f"{'='*60}\n")
    
    # Step 3: Process all 21 books sequentially (Requirement 9.3)
    for i, book_id in enumerate(TARGET_BOOKS):
        # Progress bar
        progress = (i + 1) / len(TARGET_BOOKS)
        filled = int(bar_width * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_width - filled)
        print(f"\r[{bar}] {i+1}/{len(TARGET_BOOKS)} - {book_id[:30]:<30}", end='', flush=True)
        
        # Step 4: Continue on individual book failure (Requirement 9.5)
        try:
            # Check if source files exist
            if book_id not in available_books:
                logger.warning(f"è·³è¿‡ {book_id}: ç¼ºå°‘snippetsæˆ–relationsæ–‡ä»¶")
                book_stats = BookStats(
                    book_id=book_id,
                    snippet_count=0,
                    node_count=0,
                    edge_count=0,
                    coverage=0.0,
                    orphan_snippets=[],
                    status="skipped",
                    error_message="Missing snippets or relations file"
                )
                book_stats_list.append(book_stats)
                skipped += 1
                continue
            
            # Build logic chain
            builder = LogicChainBuilder(book_id)
            chain = builder.build()
            
            # Validate
            result = validator.validate(chain)
            
            # Auto-repair if needed
            if not result.valid:
                chain, repairs = validator.auto_repair(chain, result.errors)
                result = validator.validate(chain)
            
            if not result.valid:
                # Still invalid after repair
                error_msg = "; ".join([e.message for e in result.errors[:3]])
                book_stats = BookStats(
                    book_id=book_id,
                    snippet_count=chain.metadata.snippet_count,
                    node_count=len(chain.nodes),
                    edge_count=len(chain.edges),
                    coverage=0.0,
                    orphan_snippets=[],
                    status="failed",
                    error_message=f"Validation failed: {error_msg}"
                )
                book_stats_list.append(book_stats)
                failed += 1
                continue
            
            # Write output
            writer.write(chain)
            
            # Calculate coverage and quality scores
            snippets = loader.load_snippets(book_id)
            total_snippet_ids = {s.snippet_id for s in snippets}
            
            # Calculate quality scores (Requirement 12.5)
            quality_report = quality_scorer.score(chain, total_snippet_count=len(total_snippet_ids))
            quality_scores = BookQualityScores(
                connectivity=quality_report.connectivity,
                argument_completeness=quality_report.argument_completeness,
                orphan_ratio=quality_report.orphan_ratio,
                cycle_count=quality_report.cycle_count,
                connectivity_pass=quality_report.connectivity_pass,
                argument_completeness_pass=quality_report.argument_completeness_pass,
                orphan_ratio_pass=quality_report.orphan_ratio_pass,
                overall_pass=quality_report.passes_threshold(),
            )
            
            book_stats = report_generator.create_book_stats(
                book_id=book_id,
                chain=chain,
                total_snippet_ids=total_snippet_ids,
                status="success",
                quality_scores=quality_scores,
            )
            book_stats_list.append(book_stats)
            successful += 1
            
        except FileNotFoundError as e:
            book_stats = BookStats(
                book_id=book_id,
                snippet_count=0,
                node_count=0,
                edge_count=0,
                coverage=0.0,
                orphan_snippets=[],
                status="failed",
                error_message=str(e)
            )
            book_stats_list.append(book_stats)
            failed += 1
            
        except Exception as e:
            logger.error(f"å¤„ç† {book_id} æ—¶å‡ºé”™: {e}")
            book_stats = BookStats(
                book_id=book_id,
                snippet_count=0,
                node_count=0,
                edge_count=0,
                coverage=0.0,
                orphan_snippets=[],
                status="failed",
                error_message=str(e)
            )
            book_stats_list.append(book_stats)
            failed += 1
    
    # Clear progress bar line
    print()
    
    # Step 5: Verify exactly 21 logic chain files exist (Requirement 9.4)
    output_dir = Path(OUTPUT_DIR)
    existing_chain_files = list(output_dir.glob("*.yaml")) if output_dir.exists() else []
    # Filter out build_report.md and other non-chain files
    chain_files = [f for f in existing_chain_files if not f.name.startswith("build_")]
    chain_file_count = len(chain_files)
    
    completeness_verified = chain_file_count == len(TARGET_BOOKS)
    if not completeness_verified:
        missing_count = len(TARGET_BOOKS) - chain_file_count
        report_generator.add_warning(
            f"Completeness verification failed: Expected {len(TARGET_BOOKS)} logic chain files, "
            f"found {chain_file_count} (missing {missing_count})"
        )
        logger.warning(f"å®Œæ•´æ€§éªŒè¯å¤±è´¥: æœŸæœ› {len(TARGET_BOOKS)} ä¸ªé€»è¾‘é“¾æ–‡ä»¶ï¼Œå®é™… {chain_file_count} ä¸ª")
    else:
        logger.info(f"å®Œæ•´æ€§éªŒè¯é€šè¿‡: {chain_file_count} ä¸ªé€»è¾‘é“¾æ–‡ä»¶")
    
    # Step 6: Generate and write build report with per-book summary (Requirement 9.6)
    report = report_generator.generate(book_stats_list)
    report_path = report_generator.write(report)
    
    # Print summary statistics
    print(f"\n{'='*60}")
    print(f"æ‰¹é‡æ„å»ºå®Œæˆ")
    print(f"{'='*60}")
    print(f"  âœ… æˆåŠŸ: {successful}")
    print(f"  âŒ å¤±è´¥: {failed}")
    print(f"  â­ï¸  è·³è¿‡: {skipped}")
    print(f"  ğŸ“Š æ€»è®¡: {len(TARGET_BOOKS)}")
    print(f"\n  ğŸ“ é€»è¾‘é“¾æ–‡ä»¶: {chain_file_count}/{len(TARGET_BOOKS)}")
    print(f"  {'âœ…' if completeness_verified else 'âŒ'} å®Œæ•´æ€§éªŒè¯: {'é€šè¿‡' if completeness_verified else 'å¤±è´¥'}")
    print(f"\n  ğŸ“ æ„å»ºæŠ¥å‘Š: {report_path}")
    
    # Aggregate statistics
    total_nodes = sum(s.node_count for s in book_stats_list)
    total_edges = sum(s.edge_count for s in book_stats_list)
    total_snippets = sum(s.snippet_count for s in book_stats_list)
    avg_coverage = (
        sum(s.coverage for s in book_stats_list if s.status == "success") / successful
        if successful > 0 else 0.0
    )
    
    print(f"\n  ğŸ“ˆ èšåˆç»Ÿè®¡:")
    print(f"     æ€»èŠ‚ç‚¹æ•°: {total_nodes}")
    print(f"     æ€»è¾¹æ•°: {total_edges}")
    print(f"     æ€»ç´ ææ•°: {total_snippets}")
    print(f"     å¹³å‡è¦†ç›–ç‡: {avg_coverage:.1%}")
    
    # Quality score summary (Requirement 12.5)
    books_with_quality = [s for s in book_stats_list if s.quality_scores is not None]
    if books_with_quality:
        passing_quality = sum(1 for s in books_with_quality if s.quality_scores.overall_pass)
        print(f"\n  ğŸ“Š è´¨é‡è¯„åˆ†:")
        print(f"     é€šè¿‡è´¨é‡é˜ˆå€¼: {passing_quality}/{len(books_with_quality)}")
        
        failing_books = [s for s in books_with_quality if not s.quality_scores.overall_pass]
        if failing_books:
            print(f"     âš ï¸  æœªè¾¾æ ‡ä¹¦ç±: {', '.join(s.book_id for s in failing_books[:5])}")
            if len(failing_books) > 5:
                print(f"        ... åŠå…¶ä»– {len(failing_books) - 5} æœ¬")
    
    print(f"{'='*60}\n")
    
    # Return non-zero if any failures or completeness check failed
    return 0 if (failed == 0 and completeness_verified) else 1


def cmd_validate(args) -> int:
    """
    éªŒè¯é€»è¾‘é“¾
    
    Load existing logic chain from YAML and run validation, report results.
    
    **Validates: Requirements 6.1, 6.2, 6.3, 6.4**
    """
    logger = get_logger()
    book_id = args.book_id
    
    logger.info(f"ğŸ” éªŒè¯é€»è¾‘é“¾: {book_id}")
    
    # 1. Load existing logic chain from YAML
    yaml_path = Path(OUTPUT_DIR) / f"{book_id}.yaml"
    
    if not yaml_path.exists():
        logger.error(f"âŒ é€»è¾‘é“¾æ–‡ä»¶ä¸å­˜åœ¨: {yaml_path}")
        return 1
    
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        chain = LogicChain(**data)
        logger.info(f"âœ… å·²åŠ è½½é€»è¾‘é“¾: {len(chain.nodes)} èŠ‚ç‚¹, {len(chain.edges)} è¾¹")
        
    except yaml.YAMLError as e:
        logger.error(f"âŒ YAMLè§£æé”™è¯¯: {e}")
        return 1
    except Exception as e:
        logger.error(f"âŒ åŠ è½½é€»è¾‘é“¾å¤±è´¥: {e}")
        return 1
    
    # 2. Run validation
    validator = LogicChainValidator()
    result = validator.validate(chain)
    
    # 3. Report results
    print(f"\n{'='*60}")
    print(f"é€»è¾‘é“¾éªŒè¯ç»“æœ: {book_id}")
    print(f"{'='*60}")
    
    if result.valid:
        print("âœ… éªŒè¯é€šè¿‡")
    else:
        print(f"âŒ éªŒè¯å¤±è´¥ï¼Œå‘ç° {len(result.errors)} ä¸ªé”™è¯¯:")
        for error in result.errors:
            print(f"   - [{error.error_type}] {error.message}")
    
    if result.warnings:
        print(f"\nâš ï¸  è­¦å‘Š ({len(result.warnings)}):")
        for warning in result.warnings:
            print(f"   - {warning}")
    
    # Print chain statistics
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   èŠ‚ç‚¹æ•°: {len(chain.nodes)}")
    print(f"   è¾¹æ•°: {len(chain.edges)}")
    print(f"   å…¥å£èŠ‚ç‚¹: {len(chain.entry_nodes)}")
    print(f"   ç»ˆç«¯èŠ‚ç‚¹: {len(chain.terminal_nodes)}")
    print(f"   å™äº‹é¡ºåºé•¿åº¦: {len(chain.narrative_order)}")
    print(f"{'='*60}\n")
    
    return 0 if result.valid else 1


def cmd_list(args) -> int:
    """åˆ—å‡ºæ‰€æœ‰ç›®æ ‡ä¹¦ç±"""
    logger = get_logger()
    
    print("\nğŸ“š ç›®æ ‡ä¹¦ç±åˆ—è¡¨ (21æœ¬):\n")
    
    print("ä¸­æ–‡å…¸ç± (8æœ¬):")
    for i, book in enumerate(TARGET_BOOKS[:8], 1):
        print(f"  {i:2d}. {book}")
    
    print("\nè¥¿æ–¹å…¸ç± (13æœ¬):")
    for i, book in enumerate(TARGET_BOOKS[8:], 9):
        print(f"  {i:2d}. {book}")
    
    print()
    return 0


def cmd_refine(args) -> int:
    """
    è‡ªåŠ¨ä¼˜åŒ–ä½è´¨é‡é€»è¾‘é“¾
    
    Identify chains from build_report.md with quality < threshold,
    apply SemanticClusterRefiner and IntelligentSummaryGenerator,
    run IterativeRefinementEngine on each flagged chain,
    regenerate logic chains with refinements,
    and update build_report.md with final results.
    
    **Validates: Requirements 14.1-14.5, 15.1-15.5, 16.1-16.5**
    """
    logger = get_logger()
    
    # Import required components
    from scripts.logic_chain_builder.refinement_engine import (
        IterativeRefinementEngine, QUALITY_THRESHOLD_OVERALL
    )
    from scripts.logic_chain_builder.quality_scorer import LogicChainQualityScorer
    from scripts.logic_chain_builder.report import BuildReportGenerator
    from scripts.logic_chain_builder.models import BookStats, BookQualityScores
    
    # Get threshold from args or use default
    quality_threshold = args.threshold if hasattr(args, 'threshold') and args.threshold else QUALITY_THRESHOLD_OVERALL
    max_iterations = args.max_iterations if hasattr(args, 'max_iterations') and args.max_iterations else 5
    
    logger.info(f"ğŸ”§ å¼€å§‹è‡ªåŠ¨ä¼˜åŒ–é€»è¾‘é“¾ (è´¨é‡é˜ˆå€¼: {quality_threshold:.2f}, æœ€å¤§è¿­ä»£: {max_iterations})")
    
    # Initialize components
    loader = SchemaLoader()
    validator = LogicChainValidator()
    writer = LogicChainWriter()
    quality_scorer = LogicChainQualityScorer()
    refinement_engine = IterativeRefinementEngine(
        max_iterations=max_iterations,
        quality_threshold=quality_threshold,
    )
    report_generator = BuildReportGenerator()
    
    # Step 1: Load all existing logic chains
    output_dir = Path(OUTPUT_DIR)
    if not output_dir.exists():
        logger.error(f"âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")
        print("è¯·å…ˆè¿è¡Œ 'batch' å‘½ä»¤ç”Ÿæˆé€»è¾‘é“¾")
        return 1
    
    chain_files = list(output_dir.glob("*.yaml"))
    chain_files = [f for f in chain_files if not f.name.startswith("build_")]
    
    if not chain_files:
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•é€»è¾‘é“¾æ–‡ä»¶")
        return 1
    
    logger.info(f"ğŸ“‚ å‘ç° {len(chain_files)} ä¸ªé€»è¾‘é“¾æ–‡ä»¶")
    
    # Step 2: Load chains and identify low-quality ones
    chains_to_refine = []
    all_book_stats = []
    
    print(f"\n{'='*60}")
    print(f"è‡ªåŠ¨ä¼˜åŒ–é€»è¾‘é“¾")
    print(f"{'='*60}")
    print(f"è´¨é‡é˜ˆå€¼: {quality_threshold:.2f}")
    print(f"æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_iterations}")
    print(f"{'='*60}\n")
    
    print("ğŸ“Š åˆ†æç°æœ‰é€»è¾‘é“¾è´¨é‡...")
    
    for chain_file in chain_files:
        book_id = chain_file.stem
        
        try:
            # Load chain
            with open(chain_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            chain = LogicChain(**data)
            
            # Load snippets for this book
            try:
                snippets = loader.load_snippets(book_id)
            except FileNotFoundError:
                logger.warning(f"âš ï¸  æ— æ³•åŠ è½½ {book_id} çš„snippetsï¼Œè·³è¿‡")
                continue
            
            total_snippet_count = len(snippets)
            
            # Calculate quality score
            quality = quality_scorer.score(chain, total_snippet_count)
            overall_score = quality.overall_score()
            
            if overall_score < quality_threshold:
                chains_to_refine.append((chain, snippets, quality))
                logger.info(f"  âš ï¸  {book_id}: è´¨é‡åˆ†æ•° {overall_score:.3f} < {quality_threshold:.2f}")
            else:
                logger.debug(f"  âœ… {book_id}: è´¨é‡åˆ†æ•° {overall_score:.3f} >= {quality_threshold:.2f}")
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ {book_id} å¤±è´¥: {e}")
            continue
    
    if not chains_to_refine:
        print("\nâœ… æ‰€æœ‰é€»è¾‘é“¾è´¨é‡å‡è¾¾æ ‡ï¼Œæ— éœ€ä¼˜åŒ–")
        return 0
    
    print(f"\nğŸ“‹ å‘ç° {len(chains_to_refine)} ä¸ªä½è´¨é‡é€»è¾‘é“¾éœ€è¦ä¼˜åŒ–:")
    for chain, _, quality in chains_to_refine:
        print(f"   - {chain.book_id}: {quality.overall_score():.3f}")
    
    # Step 3: Apply refinement to each low-quality chain
    print(f"\nğŸ”§ å¼€å§‹ä¼˜åŒ–...")
    
    refined_count = 0
    improved_count = 0
    refinement_results = []
    
    bar_width = 40
    
    for i, (chain, snippets, initial_quality) in enumerate(chains_to_refine):
        book_id = chain.book_id
        
        # Progress bar
        progress = (i + 1) / len(chains_to_refine)
        filled = int(bar_width * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_width - filled)
        print(f"\r[{bar}] {i+1}/{len(chains_to_refine)} - {book_id[:30]:<30}", end='', flush=True)
        
        try:
            # Run refinement engine
            result = refinement_engine.refine_until_threshold(
                chain=chain,
                snippets=snippets,
                total_snippet_count=len(snippets),
            )
            
            refined_count += 1
            
            # Check if quality improved
            if result.final_score > result.initial_score:
                improved_count += 1
            
            # Validate refined chain
            validation_result = validator.validate(result.chain)
            
            if not validation_result.valid:
                # Auto-repair if needed
                result.chain, repairs = validator.auto_repair(result.chain, validation_result.errors)
                validation_result = validator.validate(result.chain)
            
            if validation_result.valid:
                # Write refined chain
                writer.write(result.chain)
                
                refinement_results.append({
                    'book_id': book_id,
                    'initial_score': result.initial_score,
                    'final_score': result.final_score,
                    'iterations': result.iterations_performed,
                    'threshold_met': result.threshold_met,
                    'improvements': result.improvements_made,
                    'status': 'success',
                })
            else:
                refinement_results.append({
                    'book_id': book_id,
                    'initial_score': result.initial_score,
                    'final_score': result.final_score,
                    'iterations': result.iterations_performed,
                    'threshold_met': False,
                    'improvements': result.improvements_made,
                    'status': 'validation_failed',
                    'errors': [e.message for e in validation_result.errors],
                })
                
        except Exception as e:
            logger.error(f"ä¼˜åŒ– {book_id} æ—¶å‡ºé”™: {e}")
            refinement_results.append({
                'book_id': book_id,
                'initial_score': initial_quality.overall_score(),
                'final_score': initial_quality.overall_score(),
                'iterations': 0,
                'threshold_met': False,
                'improvements': [],
                'status': 'error',
                'error_message': str(e),
            })
    
    # Clear progress bar
    print()
    
    # Step 4: Regenerate build report with updated statistics
    print("\nğŸ“ æ›´æ–°æ„å»ºæŠ¥å‘Š...")
    
    # Reload all chains and regenerate report
    book_stats_list = []
    
    for book_id in TARGET_BOOKS:
        chain_path = output_dir / f"{book_id}.yaml"
        
        if not chain_path.exists():
            book_stats = BookStats(
                book_id=book_id,
                snippet_count=0,
                node_count=0,
                edge_count=0,
                coverage=0.0,
                orphan_snippets=[],
                status="skipped",
                error_message="Logic chain file not found"
            )
            book_stats_list.append(book_stats)
            continue
        
        try:
            with open(chain_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            chain = LogicChain(**data)
            
            # Load snippets
            try:
                snippets = loader.load_snippets(book_id)
                total_snippet_ids = {s.snippet_id for s in snippets}
            except FileNotFoundError:
                total_snippet_ids = set()
            
            # Calculate quality scores
            quality_report = quality_scorer.score(chain, total_snippet_count=len(total_snippet_ids))
            quality_scores = BookQualityScores(
                connectivity=quality_report.connectivity,
                argument_completeness=quality_report.argument_completeness,
                orphan_ratio=quality_report.orphan_ratio,
                cycle_count=quality_report.cycle_count,
                connectivity_pass=quality_report.connectivity_pass,
                argument_completeness_pass=quality_report.argument_completeness_pass,
                orphan_ratio_pass=quality_report.orphan_ratio_pass,
                overall_pass=quality_report.passes_threshold(),
            )
            
            book_stats = report_generator.create_book_stats(
                book_id=book_id,
                chain=chain,
                total_snippet_ids=total_snippet_ids,
                status="success",
                quality_scores=quality_scores,
            )
            book_stats_list.append(book_stats)
            
        except Exception as e:
            book_stats = BookStats(
                book_id=book_id,
                snippet_count=0,
                node_count=0,
                edge_count=0,
                coverage=0.0,
                orphan_snippets=[],
                status="failed",
                error_message=str(e)
            )
            book_stats_list.append(book_stats)
    
    # Generate and write updated report
    report = report_generator.generate(book_stats_list)
    report_path = report_generator.write(report)
    
    # Step 5: Print summary
    print(f"\n{'='*60}")
    print(f"ä¼˜åŒ–å®Œæˆ")
    print(f"{'='*60}")
    print(f"  ğŸ“Š å¤„ç†çš„ä½è´¨é‡é“¾: {len(chains_to_refine)}")
    print(f"  âœ… æˆåŠŸä¼˜åŒ–: {refined_count}")
    print(f"  ğŸ“ˆ è´¨é‡æå‡: {improved_count}")
    
    # Count how many now meet threshold
    threshold_met = sum(1 for r in refinement_results if r.get('threshold_met', False))
    print(f"  ğŸ¯ è¾¾åˆ°é˜ˆå€¼: {threshold_met}/{len(chains_to_refine)}")
    
    print(f"\n  ğŸ“ æ„å»ºæŠ¥å‘Šå·²æ›´æ–°: {report_path}")
    
    # Print detailed results
    if refinement_results:
        print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for result in refinement_results:
            status_emoji = "âœ…" if result['status'] == 'success' else "âŒ"
            score_change = result['final_score'] - result['initial_score']
            score_indicator = "â†‘" if score_change > 0 else ("â†“" if score_change < 0 else "â†’")
            threshold_indicator = "ğŸ¯" if result.get('threshold_met', False) else ""
            
            print(f"   {status_emoji} {result['book_id']}: "
                  f"{result['initial_score']:.3f} {score_indicator} {result['final_score']:.3f} "
                  f"({result['iterations']} iterations) {threshold_indicator}")
            
            if result.get('improvements'):
                for improvement in result['improvements'][:3]:  # Show first 3 improvements
                    print(f"      - {improvement[:60]}...")
                if len(result.get('improvements', [])) > 3:
                    print(f"      ... and {len(result['improvements']) - 3} more")
    
    print(f"{'='*60}\n")
    
    # Return success if at least some chains were improved
    return 0 if improved_count > 0 or len(chains_to_refine) == 0 else 1


def cmd_build_v2(args) -> int:
    """
    V2ç‰ˆæœ¬ï¼šæ„å»ºå•æœ¬ä¹¦ç±çš„é€»è¾‘é“¾
    
    ä½¿ç”¨V2ç»„ä»¶ï¼šä¸‰çº§èšç±»ã€ä¼˜å…ˆçº§è¾¹æ¨æ–­ã€è¯­ä¹‰è´¨é‡è¯„ä¼°ã€‚
    """
    logger = get_logger()
    book_id = args.book_id
    
    logger.info(f"ğŸ“š å¼€å§‹æ„å»ºé€»è¾‘é“¾ (V2): {book_id}")
    
    try:
        from scripts.logic_chain_builder.v2.builder import LogicChainBuilderV2
        
        builder = LogicChainBuilderV2(
            auto_backup=not args.no_backup,
            auto_rollback=not args.no_rollback,
        )
        result = builder.build(book_id)
        
        if result.success:
            chain = result.chain
            qr = result.quality_report
            
            print(f"\n{'='*60}")
            print(f"é€»è¾‘é“¾æ„å»ºå®Œæˆ (V2): {book_id}")
            print(f"{'='*60}")
            print(f"  èŠ‚ç‚¹æ•°: {len(chain.nodes)}")
            print(f"  è¾¹æ•°: {len(chain.edges)}")
            print(f"  å…¥å£èŠ‚ç‚¹: {len(chain.entry_nodes)}")
            print(f"  ç»ˆç«¯èŠ‚ç‚¹: {len(chain.terminal_nodes)}")
            
            if qr:
                print(f"\n  ğŸ“Š è´¨é‡æŒ‡æ ‡:")
                print(f"     è¿é€šæ€§: {qr.connectivity:.2f}")
                print(f"     å­¤ç«‹æ¯”ä¾‹: {qr.orphan_ratio:.2f}")
                print(f"     æ¨ç†è¿è´¯æ€§: {qr.reasoning_coherence:.2f}")
                print(f"     æ¡ä»¶è¦†ç›–åº¦: {qr.condition_coverage:.2f}")
                print(f"     è®ºè¯å®Œæ•´æ€§: {qr.argument_completeness:.2f}")
                print(f"     èŠ‚ç‚¹åŒè´¨æ€§: {qr.node_homogeneity:.2f}")
            
            print(f"{'='*60}\n")
            return 0
        else:
            logger.error(f"âŒ æ„å»ºå¤±è´¥: {result.error_message}")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ æ„å»ºå¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return 1


def cmd_batch_v2(args) -> int:
    """
    V2ç‰ˆæœ¬ï¼šæ‰¹é‡æ„å»ºæ‰€æœ‰ä¹¦ç±çš„é€»è¾‘é“¾
    """
    logger = get_logger()
    
    logger.info(f"ğŸ“š å¼€å§‹æ‰¹é‡æ„å»ºé€»è¾‘é“¾ (V2)")
    
    try:
        from scripts.logic_chain_builder.v2.builder import LogicChainBuilderV2
        
        builder = LogicChainBuilderV2(
            auto_backup=not args.no_backup,
            auto_rollback=not args.no_rollback,
        )
        
        results = builder.build_batch()
        
        # ç»Ÿè®¡
        successful = sum(1 for r in results.values() if r.success)
        failed = sum(1 for r in results.values() if not r.success)
        
        print(f"\n{'='*60}")
        print(f"æ‰¹é‡æ„å»ºå®Œæˆ (V2)")
        print(f"{'='*60}")
        print(f"  âœ… æˆåŠŸ: {successful}")
        print(f"  âŒ å¤±è´¥: {failed}")
        print(f"  ğŸ“Š æ€»è®¡: {len(results)}")
        print(f"\n  ğŸ“ æŠ¥å‘Šå·²ç”Ÿæˆ: data/logic_chains/build_report_v2.md")
        print(f"{'='*60}\n")
        
        return 0 if failed == 0 else 1
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ„å»ºå¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return 1


def cmd_validate_v2(args) -> int:
    """
    V2ç‰ˆæœ¬ï¼šéªŒè¯ä¹¦ç±æ•°æ®å®Œæ•´æ€§
    """
    logger = get_logger()
    book_id = args.book_id
    
    logger.info(f"ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§ (V2): {book_id}")
    
    try:
        from scripts.logic_chain_builder.v2.builder import LogicChainBuilderV2
        
        builder = LogicChainBuilderV2()
        report = builder.validate(book_id)
        
        print(f"\n{'='*60}")
        print(f"æ•°æ®éªŒè¯ç»“æœ (V2): {book_id}")
        print(f"{'='*60}")
        print(f"  å®Œæ•´æ€§: {'âœ… å®Œæ•´' if report.is_complete else 'âŒ ä¸å®Œæ•´'}")
        print(f"  Snippetæ•°é‡: {report.snippet_count}")
        print(f"  Relationæ•°é‡: {report.relation_count}")
        
        if report.missing_chapters:
            print(f"\n  âš ï¸  ç¼ºå¤±ç« èŠ‚ ({len(report.missing_chapters)}):")
            for ch in report.missing_chapters[:5]:
                print(f"     - {ch}")
            if len(report.missing_chapters) > 5:
                print(f"     ... åŠå…¶ä»– {len(report.missing_chapters) - 5} ä¸ª")
        
        if report.orphan_relations:
            print(f"\n  âš ï¸  å­¤å„¿å…³ç³» ({len(report.orphan_relations)}):")
            for rel in report.orphan_relations[:5]:
                print(f"     - {rel}")
            if len(report.orphan_relations) > 5:
                print(f"     ... åŠå…¶ä»– {len(report.orphan_relations) - 5} ä¸ª")
        
        if report.issues:
            print(f"\n  ğŸ“‹ é—®é¢˜åˆ—è¡¨ ({len(report.issues)}):")
            for issue in report.issues:
                print(f"     - [{issue.type}] {issue.description}")
                if issue.remediation:
                    print(f"       å»ºè®®: {issue.remediation}")
        
        print(f"{'='*60}\n")
        
        return 0 if report.is_complete else 1
        
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return 1


def cmd_cleanup_backups(args) -> int:
    """æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶"""
    logger = get_logger()
    
    logger.info("ğŸ—‘ï¸  æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶...")
    
    try:
        from scripts.logic_chain_builder.v2.backup import BackupManager
        
        manager = BackupManager()
        count = manager.cleanup_all_old_backups()
        
        print(f"\nâœ… å·²å½’æ¡£ {count} ä¸ªå¤‡ä»½æ–‡ä»¶åˆ° data/logic_chains/archive/\n")
        return 0
        
    except Exception as e:
        logger.error(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        return 1


def main() -> int:
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="Logic Chain Builder - ä»Schemaæ•°æ®æ„å»ºé€»è¾‘é“¾",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # V1 å‘½ä»¤
  python -m scripts.logic_chain_builder build æ»´å¤©é«“
  python -m scripts.logic_chain_builder batch
  python -m scripts.logic_chain_builder validate æ»´å¤©é«“
  python -m scripts.logic_chain_builder list
  
  # V2 å‘½ä»¤ (æ¨è)
  python -m scripts.logic_chain_builder build-v2 æ»´å¤©é«“
  python -m scripts.logic_chain_builder batch-v2
  python -m scripts.logic_chain_builder validate-v2 æ»´å¤©é«“
  python -m scripts.logic_chain_builder cleanup-backups
        """
    )
    
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º"
    )
    
    parser.add_argument(
        "--log-file",
        type=str,
        help="æ—¥å¿—æ–‡ä»¶è·¯å¾„"
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å­å‘½ä»¤")
    
    # build å‘½ä»¤
    build_parser = subparsers.add_parser("build", help="æ„å»ºå•æœ¬ä¹¦ç±çš„é€»è¾‘é“¾")
    build_parser.add_argument("book_id", help="ä¹¦ç±ID")
    build_parser.set_defaults(func=cmd_build)
    
    # batch å‘½ä»¤
    batch_parser = subparsers.add_parser("batch", help="æ‰¹é‡æ„å»ºæ‰€æœ‰ä¹¦ç±çš„é€»è¾‘é“¾")
    batch_parser.set_defaults(func=cmd_batch)
    
    # validate å‘½ä»¤
    validate_parser = subparsers.add_parser("validate", help="éªŒè¯é€»è¾‘é“¾")
    validate_parser.add_argument("book_id", help="ä¹¦ç±ID")
    validate_parser.set_defaults(func=cmd_validate)
    
    # list å‘½ä»¤
    list_parser = subparsers.add_parser("list", help="åˆ—å‡ºæ‰€æœ‰ç›®æ ‡ä¹¦ç±")
    list_parser.set_defaults(func=cmd_list)
    
    # refine å‘½ä»¤
    refine_parser = subparsers.add_parser("refine", help="è‡ªåŠ¨ä¼˜åŒ–ä½è´¨é‡é€»è¾‘é“¾")
    refine_parser.add_argument(
        "--threshold",
        type=float,
        default=0.6,
        help="è´¨é‡é˜ˆå€¼ (é»˜è®¤: 0.6)"
    )
    refine_parser.add_argument(
        "--max-iterations",
        type=int,
        default=5,
        help="æœ€å¤§è¿­ä»£æ¬¡æ•° (é»˜è®¤: 5)"
    )
    refine_parser.set_defaults(func=cmd_refine)
    
    # V2 å‘½ä»¤
    # build-v2 å‘½ä»¤
    build_v2_parser = subparsers.add_parser("build-v2", help="[V2] æ„å»ºå•æœ¬ä¹¦ç±çš„é€»è¾‘é“¾")
    build_v2_parser.add_argument("book_id", help="ä¹¦ç±ID")
    build_v2_parser.add_argument(
        "--no-backup",
        action="store_true",
        help="ç¦ç”¨è‡ªåŠ¨å¤‡ä»½"
    )
    build_v2_parser.add_argument(
        "--no-rollback",
        action="store_true",
        help="ç¦ç”¨å¤±è´¥æ—¶è‡ªåŠ¨å›æ»š"
    )
    build_v2_parser.set_defaults(func=cmd_build_v2)
    
    # batch-v2 å‘½ä»¤
    batch_v2_parser = subparsers.add_parser("batch-v2", help="[V2] æ‰¹é‡æ„å»ºæ‰€æœ‰ä¹¦ç±çš„é€»è¾‘é“¾")
    batch_v2_parser.add_argument(
        "--no-backup",
        action="store_true",
        help="ç¦ç”¨è‡ªåŠ¨å¤‡ä»½"
    )
    batch_v2_parser.add_argument(
        "--no-rollback",
        action="store_true",
        help="ç¦ç”¨å¤±è´¥æ—¶è‡ªåŠ¨å›æ»š"
    )
    batch_v2_parser.set_defaults(func=cmd_batch_v2)
    
    # validate-v2 å‘½ä»¤
    validate_v2_parser = subparsers.add_parser("validate-v2", help="[V2] éªŒè¯ä¹¦ç±æ•°æ®å®Œæ•´æ€§")
    validate_v2_parser.add_argument("book_id", help="ä¹¦ç±ID")
    validate_v2_parser.set_defaults(func=cmd_validate_v2)
    
    # cleanup-backups å‘½ä»¤
    cleanup_parser = subparsers.add_parser("cleanup-backups", help="æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶")
    cleanup_parser.set_defaults(func=cmd_cleanup_backups)
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    log_level = logging.DEBUG if args.verbose else logging.INFO
    setup_logging(level=log_level, log_file=args.log_file)
    
    if args.command is None:
        parser.print_help()
        return 0
    
    return args.func(args)


if __name__ == "__main__":
    sys.exit(main())
